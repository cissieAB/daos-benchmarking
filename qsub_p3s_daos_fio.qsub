#!/bin/bash

#PBS -A e2sar-daos
#PBS -l select=1
#PBS -l walltime=5:59:00
#PBS -q prod
#PBS -l filesystems=home:flare:daos_user_fs
#PBS -j oe

set -e  # Exit on any error

# Variables
POOL_NAME=e2sar
CONTAINER_BASE_NAME="${USER}-fio_test-p3s"
RESULTS_DIR="./fio_results_p3s_$(date +%Y-%m-%d_%H-%M-%S)"
LOG_FILE="$RESULTS_DIR/logs.txt"

NUMA_CPU_CORES=4,9,14,19,20,25,56,61,66,71,74,79

# Values for --bs
BLOCK_SIZES=("4K" "16K" "1M" "2M" "4M")
# BLOCK_SIZES=("2M")              # mini for testing

# Keys for setting --rw and --rwmixread
# RW_CATEGORIES=("SeqR" "SeqRH" "SeqBal" "SeqWH" "SeqW" "RandR" "RandRH" "RandBal" "RandWH" "RandW")
RW_CATEGORIES=("SeqR" "SeqRH" "SeqBal" "SeqWH" "SeqW") # sequential only
# RW_CATEGORIES=("SeqR" "SeqRH")  # mini for testing

# Values for --numjobs
# NUM_JOBS_SIZES=("16" "32" "64" "128")
NUM_JOBS_SIZES=("64")            # p3

# Values for --iodepth
IO_DEPTHS=("1" "8" "16" "32" "64" "128") # full
# IO_DEPTHS=("32")                # mini for testing

get_fio_rw_params() {
    local category="$1"
    case "$category" in
        "SeqR")     echo "--rw=read                     --unified_rw_reporting=0" ;;
        "SeqRH")    echo "--rw=rw       --rwmixread=80  --unified_rw_reporting=1" ;;
        "SeqBal")   echo "--rw=rw                       --unified_rw_reporting=1" ;;
        "SeqWH")    echo "--rw=rw       --rwmixread=20  --unified_rw_reporting=1" ;;
        "SeqW")     echo "--rw=write                    --unified_rw_reporting=0" ;;
        "RandR")    echo "--rw=randread                 --unified_rw_reporting=0" ;;
        "RandRH")   echo "--rw=randrw   --rwmixread=80  --unified_rw_reporting=1" ;;
        "RandBal")  echo "--rw=randrw                   --unified_rw_reporting=1" ;;
        "RandWH")   echo "--rw=randrw   --rwmixread=20  --unified_rw_reporting=1" ;;
        "RandW")    echo "--rw=randwrite                --unified_rw_reporting=0" ;;
        *) error "ERROR: Unknown category $category"; exit 1 ;;
    esac
}

error() {
    echo  "[ERROR] $1" >> $LOG_FILE
}

success() {
    echo "[SUCCESS] $1" >> $LOG_FILE
}

warning() {
    echo "[WARNING] $1" >> $LOG_FILE
}

# Cleanup function
cleanup() {
    local pool_name="$1"
    local container_name="$2"
    mount_point=/tmp/"$pool_name"/"$container_name"
    
    # Remove mounting point on a compute node
    clean-dfuse.sh $pool_name:$container_name || warning "clean-dfuse.sh failed"

    # Remove mount point
    if [ -d "$mount_point" ]; then
	rmdir "$mount_point" || warning "Failed to remove directory $mount_point"
    fi

    # Destroy container
    daos container destroy "$pool_name" "$container_name" || warning "Failed to destroy container $container_name"
    
    # Output log of existing pools
    daos container list "$pool_name" >> $LOG_FILE
}

# Create results directory.
# On compute node, this will be in /$HOME
mkdir -p $RESULTS_DIR

# Load DAOS module
module use /soft/modulefiles/ || { error "Failed to use modulefiles"; exit 1; }
module load daos || { error "Failed to load DAOS module"; exit 1; }

# Verify pool exists
daos pool list | grep -q -- "$POOL_NAME" || { error "No pool: $POOL_NAME"; exit 1; }

combined_json="${RESULTS_DIR}/combined.json"
echo '{"jobs":[]}' > "$combined_json"

for iod in "${IO_DEPTHS[@]}"; do
    for nj in "${NUM_JOBS_SIZES[@]}"; do
        for bs in "${BLOCK_SIZES[@]}"; do
            for rw_cat in "${RW_CATEGORIES[@]}"; do

                RW_PARAMS=$(get_fio_rw_params "$rw_cat")
                CASE_ID="${rw_cat}-${bs}-${nj}-${iod}"
                CONTAINER_NAME="${CONTAINER_BASE_NAME}_${CASE_ID}"
                MOUNT_POINT="/tmp/$POOL_NAME/$CONTAINER_NAME"
                
                # Create container
                if ! daos container create --type=POSIX "$POOL_NAME" "$CONTAINER_NAME" --properties=rd_fac:1; then
                    error "Failed to create container $CONTAINER_NAME"
                    continue
                fi
                
                # Mount container to /tmp/$POOL_NAME/$CONTAINER_NAME (system default)
                launch-dfuse.sh "$POOL_NAME:$CONTAINER_NAME"

                # Build FIO command
                fio_cmd=" taskset -c $NUMA_CPU_CORES fio "
                fio_cmd+=" --output=${RESULTS_DIR}/fio_${CASE_ID}.json"
                fio_cmd+=" --output-format=json"
                fio_cmd+=" --warnings-fatal"
                fio_cmd+=" --name=${CASE_ID}"
                fio_cmd+=" --numjobs=${nj}"
                fio_cmd+=" --runtime=60 --time_based"
                fio_cmd+=" --directory=${MOUNT_POINT}"
                fio_cmd+=" --direct=1 --buffered=0"
                fio_cmd+=" ${RW_PARAMS}"
                fio_cmd+=" --randrepeat=0 --norandommap"
                fio_cmd+=" --bs=${bs}"
                fio_cmd+=" --refill_buffers"
                fio_cmd+=" --size=128M"
                fio_cmd+=" --ioengine=pvsync"
                fio_cmd+=" --iodepth=${iod}"
                fio_cmd+=" --group_reporting"
                fio_cmd+=" --lat_percentiles=1"
                # fio_cmd+=" --percentile_list=90:95:99:99.5:99.9"

                ${fio_cmd} || { error "$fio_cmd failed"; exit 1; }
                success "FIO benchmark completed for ${CASE_ID}"

		        jq -s '{ jobs: (.[0].jobs + .[1].jobs) }' "$combined_json" <(grep -v 'clock setaffinity' "${RESULTS_DIR}/fio_${CASE_ID}.json") > "${combined_json}.tmp" 
                mv "${combined_json}.tmp" "$combined_json"
                rm "${RESULTS_DIR}/fio_${CASE_ID}.json"

                # cat "${RESULTS_DIR}/fio_result_${CASE_ID}.csv" >> "${RESULTS_DIR}/fio_result_combined.csv"
                # rm "${RESULTS_DIR}/fio_result_${CASE_ID}.csv"
                
                # Cleanup
                cleanup "$POOL_NAME" "$CONTAINER_NAME" "$MOUNT_POINT"
            done
        done
    done
done
