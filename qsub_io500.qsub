#!/bin/bash

#PBS -l walltime=5:59:00
#PBS -l select=8
#PBS -A e2sar-daos
#PBS -q prod
#PBS -l filesystems=home:flare:daos_user_fs
#PBS -j oe

# Load DAOS module
module use /soft/modulefiles/ || { error "Failed to use modulefiles"; exit 1; }
module load daos || { error "Failed to load DAOS module"; exit 1; }

# Variables
POOL_NAME=e2sar
export DAOS_POOL=${POOL_NAME}
NNODES=`wc -l < $PBS_NODEFILE`
LOCAL_HOSTFILE="${HOME}/local.hostfile"
cat $PBS_NODEFILE > "$LOCAL_HOSTFILE"

CONTAINER_NAME="${USER}-io500"
export DAOS_CONT=${CONTAINER_NAME}
MOUNT_POINT="/tmp/${POOL_NAME}/${CONTAINER_NAME}"

# Check if container exists
if daos cont query "$POOL_NAME" "$CONTAINER_NAME" >/dev/null 2>&1; then
    echo "Container $CONTAINER_NAME already exists, destroying it..."
    if ! daos container destroy "$POOL_NAME" "$CONTAINER_NAME"; then
        error "Failed to destroy container $CONTAINER_NAME"
        exit 1
    fi
else
    echo "Container $CONTAINER_NAME does not exist, will create a new one."
fi
# Create container
if ! daos container create --type=POSIX "$POOL_NAME" "$CONTAINER_NAME" --properties=rd_fac:1; then
    error "Failed to create container $CONTAINER_NAME"
    exit 1
fi
echo "Get container properties:"
daos container get-prop $POOL_NAME $CONTAINER_NAME

launch-dfuse.sh "$POOL_NAME:$CONTAINER_NAME"

if ! mount | grep "$MOUNT_POINT"; then
  echo "ERROR: dfuse did not mount properly at $MOUNT_POINT"
  exit 1
fi


# Number of nodes and ranks per node 
RANKS_PER_NODE=12
NRANKS=$((NNODES * RANKS_PER_NODE))

CPU_BINDING1=list:4:9:14:19:20:25:56:61:66:71:74:79
# skip-1 breadth scan. Bind 48 cores
CPU_BINDING_SKIP1="list:4:6:56:58:9:11:61:63:12:14:64:66:17:19:69:71:20:22:72:74:25:27:77:79:28:30:80:82:33:35:85:87:36:38:88:90:41:43:93:95:44:46:96:98:49:51:100:102"
export ZE_FLAT_DEVICE_HIERARCHY=COMPOSITE
export AFFINITY_ORDERING=compact


RESULTS_DIR="$HOME/io500_${PBS_JOBID%%.*}_${NNODES}-n"
mkdir -p ${RESULTS_DIR}

IO500_WORKDIR="${MOUNT_POINT}/io500_data"
mkdir -p ${IO500_WORKDIR}

cd $HOME/io500
export FI_UNIVERSE_SIZE=16383
export FI_OFI_RXM_USE_SRX=1
export LD_LIBRARY_PATH="/usr/lib64/:$LD_LIBRARY_PATH"
export I_MPI_OFI_LIBRARY_INTERNAL=0

CONFIG_FILE="${HOME}/myconfig.ini"
cat > "$CONFIG_FILE" << EOF
[global]
datadir = ${IO500_WORKDIR}
timestamp-datadir = FALSE
# No timestamps in case the date changes while the job is running
resultdir = ${RESULTS_DIR}
timestamp-resultdir = FALSE
API = DFS --dfs.pool=${POOL_NAME} --dfs.cont=${CONTAINER_NAME}
# API = POSIX
# The verbosity level between 1 and 10
verbosity = 1
# Type of packet that will be created [timestamp|offset|incompressible|random]
dataPacketType = timestamp


[debug]
# For a valid result, the stonewall timer must be set to the value according to the rules. If smaller INVALIDATES RUN; FOR DEBUGGING.
stonewall-time = 300
# Pause between phases while in this directory lies a file with the phase name, e.g., easy-create. This can be useful for performance testing, e.g., of tiered storage. At the moment it INVALIDATES RUN; FOR DEBUGGING.
pause-dir = 

[ior-easy]
# The API to be used
API = DFS --dfs.pool=${POOL_NAME} --dfs.cont=${CONTAINER_NAME} --dfs.dir_oclass=RP_2GX --dfs.oclass=RP_2GX
# API = POSIX
# Transfer size
transferSize = 2m
# Block size; must be a multiple of transferSize
blockSize = 9920000m
# Create one file per process
filePerProc = TRUE
# Use unique directory per file per process
uniqueDir = FALSE
# Run this phase
run = TRUE
# The verbosity level
verbosity = 

[mdtest-easy]
# The API to be used
API = DFS --dfs.pool=${POOL_NAME} --dfs.cont=${CONTAINER_NAME} --dfs.dir_oclass=RP_2GX --dfs.oclass=RP_2G1
# API = POSIX
# Files per proc
n = 1000000
# Run this phase
run = TRUE

[ior-hard]
# The API to be used
API = DFS --dfs.pool=${POOL_NAME} --dfs.cont=${CONTAINER_NAME} --dfs.dir_oclass=RP_2G1 --dfs.oclass=RP_2GX --dfs.chunk_size=470080
# API = POSIX
# Number of segments
segmentCount = 10000000
# Collective operation (for supported backends)
collective = 
# Run this phase
run = TRUE
# The verbosity level
verbosity = 

[mdtest-hard]
# The API to be used
API = DFS --dfs.pool=${POOL_NAME} --dfs.cont=${CONTAINER_NAME} --dfs.dir_oclass=RP_2GX --dfs.oclass=RP_2G1
# API = POSIX
# Files per proc
n = 1000000
# File limit per directory (MDTest -I flag) to overcome file system limitations INVALIDATES RUN; FOR DEBUGGING.
files-per-dir = 
# Run this phase
run = TRUE


[find]
# Set the number of processes for pfind/the external script
nproc = ${NRANKS}
# Run this phase
run = TRUE

EOF

# Run IO500
mpiexec --env LD_PRELOAD=/usr/lib64/libpil4dfs.so --hostfile ${LOCAL_HOSTFILE} \
 -np ${NRANKS} -ppn ${RANKS_PER_NODE} --cpu-bind ${CPU_BINDING_SKIP1} \
 --no-vni -genvall -- \
 ${HOME}/io500/io500.sh ${CONFIG_FILE}



# Cleanup
fusermount3 -u "$MOUNT_POINT" || warning "Failed to unmount $MOUNT_POINT"
    sleep 2

# Remove mount point
if [ -d "$MOUNT_POINT" ]; then
    rmdir "$MOUNT_POINT" || warning "Failed to remove directory $MOUNT_POINT"
fi

# Destroy container
daos cont destroy $POOL_NAME $CONTAINER_NAME || warning "Failed to destroy container $CONTAINER_NAME"
